---
title: "week2notes"
format: html
---

# Week 2 Lecture Notes

## Lecture 3 - Geocentric Models

Our metaphor here is how we explain the patterns of planet's orbits. Historically they managed to make some genuinely wild guesses at the shapes of planetary orbits as being based around them orbiting around earth that matched how they are observed from earth. but those are fully wrong - this is setting the stage to teach us linear regression.

### About Linear Regression

Linear regression is geocentric: it describes associations, makes predictions, but it's almost always mechanistically wrong. The problem is not the model itself but we can't believe it's a generative structural model of reality. They're not accurate mechanistic explanations of reality.

We're going to use gaussian error models - the error model is not generative in the same detailed way we were doing generative models last week but it will be very useful for us.

#### More on Gaussian (normal) distributions

a lot of natural processes produce something very much like a gaussian distribution, b/c:

1.  they're generative - summed fluctuations in small natural processes tend towards normal distributions

2.  inferential - for estimating mean and variance, normal distribution is the least informative distribution (it contains no other info beyond the mean and variance - so if you know more about that then you could use other distributions, but gaussian is making the least assumptions if you don't)

a variable does not have to be empirically normally distributed for normal model to be useful because it contains the fewest assumptions - it is a good tool for estimating mean and variance.

### Making Geocentric Models

We're going to develop some skills:

1.  language for representing models

2.  calculate posterior distributions for multiple unknowns

3.  we'll better understand constructing linear models

revisiting our owl:

1.  state a clear question

2.  sketch your causal assumptions (DAG)

3.  use the sketch to define a generative model

4.  use generative model to build estimator (garden of forking data - planted on generative assumptions)

5.  profit (analyze the real data)

or:

for human growth today, we'll:

1.  question/goal/estimand

2.  scientific model

3.  statistical models

4.  validate model

5.  analyze data

#### Question/Goal/estimand

Describe association between Adult weight and height

```{r}
library(rethinking)
data(Howell1)
```

#### Scientific Model

How does height influence weight?

H -\> W

or, W = f(H)

Both mean, weight is some function of height

#### Statistical model

options for this:

1.  dynamic models - incremental growth of an organism, both mass and height derive from growth pattern, gaussian variation result of summed fluctuations. We're NOT going to do this

2.  Static models - changes in height result in changes in weight, but no mechanism, gaussian variation result of growth history. This is what we're going to do.

So, for us, How does height influence weight?

H -\> W \<- U

W = f(H,U)

this is, Weight is some function of height and unobserved stuff

Now, building the model:

For adults, weight is a proportion of height plus the influence of unobserved causes

![](images/Screenshot%202025-02-24%20at%2017.20.19.png)

```{r}
#function to simulate weights
sim_weight <- function(H, b, sd) { #b for beta - some proportion of height 
  U <- rnorm(length(H), 0, sd)
  W <- b * H + U
  return(W)
}
```

```{r}
H <- runif (200, min = 130, max = 170)
W <- sim_weight(H, b = 0.4, sd = 5)
plot(W~H, col = 2, lwd = 3)
```

Now we need some language about statistical model notation:

1.  list the variables

2.  define each variable as a deterministic or distributional function of the other variables (where it gets its value)

For our human height model above:

![](images/Screenshot%202025-02-24%20at%2017.28.13.png)

"i"s represent individuals

= is deterministic

\~ is distributed as

first equation is for expected weight

second says the unknown stuff has a gaussian distribution with standard error sigma

third says height is uniformly distributed from 130 to 170

##### Developing an estimator:

We want to estimate how the avg weight changes with height:

![](images/Screenshot%202025-03-03%20at%2016.12.58.png)

we start by building a posterior distribution:

![](images/Screenshot%202025-03-03%20at%2016.19.14.png)

left is posterior probability of a specific line - alpha and beta define the line, sigma defines the eror around it - these are condition on the data which are H and W which are our observed data. we need a posterior distribution for alpha beta and sigma bc they're unobserved, we don't for H and W because those are our observed data

on the right, we have the number of ways the generative process could produce W sub i time the prior (previous data). then there's the normalizing constant which is Z and apparently we're not fussing with it.

Through the first half of the course we're going to use quadratic approximation instead of grid approximation (but I think eventually we'll move to MCMC). quadratic is continuous - we approximate the posterior distribution as a multivariate gaussian distribution (bc posterior distributions are typically gaussian if the sample is large enough!)

we'll use the quap function from the rethinking package

m3.1 \<- quap(

alist(

W \~ dnorm(mu, sigma),

mu \<- a+ b\*h,

a \~ dnorm (0,10),

b \~dunif(0,1),

sigma \~dunif(0,10)

) , data = list(W=W, H=H)

)

```{r}
m3.1 <- quap(
  alist(
    W ~ dnorm(mu, sigma),
    mu <- a+ b*H,
    a ~ dnorm (0,10),
    b ~dunif(0,1),
    sigma ~dunif(0,10)
  ) , data = list(W=W, H=H)
)
```

Now we have to define the priors so that before the model has seen data it doesn't make wild claims. priors should express scientific knowledge softly. we do these like so for this example:

![](images/Screenshot%202025-03-03%20at%2016.32.31.png){width="458"}

we defined these using the following logical statements. We're basically trying to bumper it in to reality as much as possible without telling it fully what the model should do.

When H=0, W = 0 (that's alpha varying from 0-10) - giving it a bit of a range there but keeping it not crazy

Weight increases (on avg) with height - beta has to be positive

weight(kg) is less than height (cm)

sigma must be positive - standard deviations are positive, a standard deviation of 10 would be a 100kilo variation around a point which is super high so it should be within that range.

now we're going to understand the implications of the priors thru simulation - what do the observable variables look like with these priors?

```{r}
n <- 1e3 #a thousand random samples from the prior of alpha and beta
a <-rnorm(n,0,10) # normally distributed mean zero std 10
b <- runif(n,0,1) # normally distributed btwn 0-1
plot( NULL, xlim = c(130, 170) , ylim = c(50,90),
      xlab = "height(cm)", ylab = "weight(kg)") #plot the implied line
for(j in 1:50) abline (a = a[j], b = b[j], lwd = 2, col = 2) #add fifty points in
```

This is what the golem imagines before it's seen any data based on the priors we gave it. The intercept is just pretty nutty in the above. doing this helps us understand what the model is thinking.

We're going to work with these kinda bad priors for now but we'll come back to it later - these won't do any damage for now.

NOTES ON PRIORS!

-   there are no correct priors, only scientifically justifiable ones

-   justify with information outside the data - like we do with the rest of the model

-   priors are not so important in simple models

-   very important/useful in complex models

-   need to practice now - simulate and understand them!

so, now we have a generative model that we can use to test it on simulated people!

#### Validate Model

we HAVE to test our code like this. test statistical model with simulated observations from scientific model. Our golem might be broken and even working golems might not distribute what we hoped.

```{r}
#simulate a sample of 10 peole
set.seed(93)
H <- runif(10,130,170)
W <- sim_weight(H, b=0.5, sd=5) #data generating slope is 0.5, we'll vary that across runs and make sure the estimator tracks it. 

#run the model using quap, takes formula list 
library(rethinking)
m3.1 <- quap(
  alist(
    W ~ dnorm(mu, sigma),
    mu <- a+ b*H,
    a ~ dnorm (0,10),
    b ~dunif(0,1),
    sigma ~dunif(0,10)
  ) , data = list(W=W, H=H)
)

#summary
precis(m3.1) #this just gives a quick summmary of the marginal posterior distributions of the unknown
```

you would want to rerun that with different slopes and see if the posterior distribution tracks it

#### Analyze data

```{r}
data(Howell1)
d2 <- Howell1[Howell1$age>=18,]
```

```{r}
dat <- list(W=d2$weight, H=d2$height)
m3.2 <- quap(
  alist(
    W ~ dnorm(mu, sigma),
    mu <- a+ b*H,
    a ~ dnorm (0,10),
    b ~dunif(0,1),
    sigma ~dunif(0,10)
  ) , data = list(W=W, H=H)
)
precis(m3.2)
```

my summary stats are pretty different from his? not totally sure why: here were his:

![](images/Screenshot%202025-03-03%20at%2017.05.29.png){width="412"}

first law of statistical interpretation: the parameters are not independent of one another and cannot always be independently interpreted. instead, we push out posterior predictions and describe/interpret those. We do that by extracting samples.

The posterior distribution is full of lines:

```{r}
post <-  extract.samples(m3.2)
plot(d2$height, d2$weight, col =2, lwd = 3,
     xlab = "height (cm)", ylab = "weight (kg)")
for(j in 1:20)
  abline(a = post$a[j], b=post$b[j], lwd =1) #not sure why these aren't showing up on the plot below
```

there's no one true line in bayes analysis - better way is to draw all the lines and plot them all.

the posterior is also full of people. we can add a prediction envelop around the solid lines in the graph that helps us convey the uncertainty? this is a percentile interval - we give it a fit model and some new data - 20 individuals ranging from 130 to 190 in height. it simulates from the posterior distribution the posterior predictive animation behind the scenes and then returns a big data table of the simulations which would be individual weights, takes a percentile interval for each height value - I **really don't know what it's doing below at all actually.**

```{r}
height_seq <- seq(130, 190, len=20)
W_postpred <- sim(m3.2,
                  data = list(H=height_seq))
W_PI <- apply(W_postpred, 2, PI)
lines(height_seq, W_PI[1,], lty=2, lwd = 2)
lines(height_seq, W_PI[2,], lty = 2, lwd = 2)
```

## Lecture 4 - Categories and Curves

In this lecture, for a single generative model we're going to have a generative model with mulitple estimands for the first time. This means multiple estimators. We also are going to process results differently - the result we want is not going to show up in a summary table so we're going to need to do some post-processing.

the new statistical tools in this lecture are categorical models and curves - this are tools we need to build causal estimators.

### Categories

How to cope with causes that are not continuous - discrete, unordered types of data. In linear models that normally means you fit a different line for each of the categories:

In the Howell height and weight data, there are also columns for age and sex. if we wanted to add sex in, we'd start by adding it to the DAG:

![](images/Screenshot 2025-03-05 at 15.41.54.png){width="483"}

The causes aren't in the data. you can't just look at the scatterplot and know what causes what! we KNOW that height influences weight and that there are few things you can do to adjust weight that would influence height. similarly, height doesn't influence sex, but sex does influence height. So:

![](images/Screenshot 2025-03-05 at 15.44.09.png){width="323"}

the influence of sex on weight is both direct and indirect, because it also influences height. SO:

![](images/Screenshot 2025-03-05 at 15.45.05.png){width="264"}

(height is a function of sex, weight is a function of height and sex)

there are also unobserved causes that influence each variable - we normally don't draw them in and can ignore them UNLESS they're SHARED (confounding variables) - for example, if temperature in reptiles influences both sex and weight, that would be an issue. we'll come back to that later but for our human example we are ignoring confounds.

here's a simple version of a function that we can use to simulate height and weight with inputs of sex data

```{r}
#S=1 female; S=2 male
sim_HW <- function(S,b,a) { #sex, b is beta, proportionality of height that is weight, and a, which is direct effect of sex 
  N <- length(S) 
  H <- ifelse(S==1, 150, 160) +rnorm(N,0,5) #if female, on avg, height 150cm, if male, 160, add some gaussian variation
  W <-a[S] +b[S]*H + rnorm(N,0,5) #same as prev lecture - but adding, men and women could have different proportionality with height (for each S, diff height and weight line)
  data.frame(S,H,W)
}

S <-  rbern(100) +1 #100 individuals randomly males or females
dat <- sim_HW(S, b=c(0.5, 0.6), a = c(0,0)) #simulate height and weight w no direct effect of sex on bodyweight and there's slightly different slopes
head(dat)
```

First think scientifically and define questions:

different causal questions need different statistical models:

-   Q: causal effect of H on W?

-   Q: causal effect of S on W?

-   Q: direct causal effect of S on W?

We'll do all:

![](images/Screenshot 2025-03-05 at 15.59.30.png){width="485"}

Drawing categorical owls:

several wasys to code categorical variables:

1\) indicator variables (0/1)

2\) index variables: 1,2,3,4. this are like ID numbers.

We'll use index variables: they extend to many categories with no change in code, they're better for specifying priors, and extend well to multi-level models.

![](images/Screenshot 2025-03-05 at 16.02.48.png){width="385"}

![](images/Screenshot 2025-03-05 at 16.04.27.png){width="381"}

I got a little lost as he explained index variables

### Testing with categories

#### Total causal effect of sex

What is the total causal effect of sex? it is something we measure from simulation in this because there's direct and indirect influence. We can simulate this because we have a generative model that we can do experiments on. so we would intervene on a single variable - we could construct two samples, one where everyone is male and one where everyone is female and then caluculate the avg difference in body weight to get the total causal effect of sex on body weight (includes direct and indirect). that's shown below:

![](images/Screenshot 2025-03-05 at 16.12.10.png){width="296"}

```{r}
#female sample
S <-  rep(1,100)
simF <- sim_HW(S, b=c(0.5, 0.6), a = c(0,0))

#male sample 
S <- rep(2,100)
simM <- sim_HW(S, b=c(0.5,0.6), a = c(0,0))

#effect of sex (male-female)
mean(simM$W - simF$W)
```

now run the estimating model and synthetic sample. the estimator is the model that we developed with the indicator variable. We're doing an observational study where sex is a random variable, we observe their height and weight and estimate the causal effect of sex through both direct and indirect maths.

```{r}
#observe sample
S <-rbern(100) +1
dat <- sim_HW(S,b=c(0.5, 0.6), a=c(0,0))

#estimate posterior
m_SW <- quap(
  alist(
    W ~ dnorm(mu, sigma),
    mu <- a[S],
    a[S] ~ dnorm(60,10),
    sigma ~ dunif(0,10)
  ), data = dat
)
precis(m_SW, depth = 2)
```

output at the bottom has 2 a variables, one for men one for women, there's a different average weight for each, the difference between these two is the combined direct and indirect effect of sex on weight.

##### Analyze the real sample:

```{r}
data(Howell1)
d <- Howell1
d <-d[d$age>=18,]
dat <- list(
  W = d$weight,
  S=d$male + 1 #S = 1 female, S = 2 male
) 

m_SW <- quap(
  alist(
    W ~dnorm(mu,sigma),
    mu <- a[S],
    a[S] ~ dnorm(60,10),
    sigma ~ dunif(0,10)
  ), data = dat
)
```

![](images/Screenshot 2025-03-05 at 16.24.58.png){width="276"}

Now postprocessing:

usually w/ these models the unknowns aren't what we want - we don't want a(1) or a(2), we want to do comparisons of the posterior distributions. think about difference in mean weight. that's not in the posterior, but we're looking at the contrast between the two categories. we need to use the whole posterior distribution for that. second chunk of code is simulating normally distributed PEOPLE using sigma. in the second plot there's a lot more overlap - the difference in means is reliable but there's a lot of overlap in mean weights.

```{r}
#posterior mean W
post <- extract.samples(m_SW)
dens (post$a[,1], xlim = c(39,50), lwd=3,
      col = 2, xlab = "posterior mean weight(kg)")
dens(post$a[,2], lwd = 3, col = 4, add = TRUE)

#posterior W distributions
W1 <-  rnorm(1000, post$a[,1], post$sigma)
W2 <- rnorm(1000, post$a[,2], post$sigma)
dens(W1, xlim=c(20,70), ylim=c(0,0.085),
     lwd=3, col = 2)
dens(W2, lwd = 3, col = 4, add = TRUE)
```

we still now want to compute the posterior distribution of the difference between category 1 and 2 -

we need to always compute the contrast, or difference between the categories. it is never legitimate to compare overlap, only difference. so, we can here compute the causal difference in **means**:

```{r}
#causal contrast in means
mu_contrast <- post$a[,2] - post$a[,1]

dens(mu_contrast, xlim = c(3,10), lwd = 3, col = 1, xlab = "posterior mean weight contrast (kg)")
```

That's the posterior distribution of difference between women and men in this sample of body weight - so difference is somewhere between 5 and 9

What about **real people**? the distributions overlap a lot, what's the contrast between these distributions?

```{r}
#posterior W distributions
W1 <-  rnorm(1000, post$a[,1], post$sigma)
W2 <- rnorm(1000, post$a[,2], post$sigma)

#contrast
W_contrast <- W2 - W1
dens(W_contrast, xlim = c(-25,35), lwd=3,
     col = 1, xlab = "posterior weight contrast (kg")

#proportion above zero
sum(W_contrast >0)/1000
#proportion below zero
sum(W_contrast <0) /1000
```

so the proportion of men that are heavier than women is 82%, or the amount of the distribution that's above zero.

#### Direct causal effect of sex on height

SO! all the above was figuring out the total causal effect of sex on weight. NOW, what about DIRECT causal effect of S on W? so we need to try to "block" the indirect effect through height. we're calling this "stratifying by height"

```{r}
S <- rbern(100) +1
dat <- sim_HW(S, b=c(0.5,0.5), a=c(0,10))
```

in this simulation the causal effect of height is the same for men on women (same slopes). the direct effects are different - men on avg 10 kilos heavier regardless of height in the above simulation. the indirect effect would be that height influences weight differently for sexes (so, different slopes), so we're removing that. the direct effect is that weight differs based on sex directly.

So how can we make a statistical model that estimates that direct effect? another linear model. We're going to update our model from before to stratify for the height effect. "for individuals of the same height, what are the differences by category"

![](images/Screenshot 2025-03-05 at 17.03.08.png)

![](images/Screenshot 2025-03-05 at 17.04.28.png)

so stratify by categorical variables we're going to put sex subscripts on everthing.

![](images/Screenshot 2025-03-05 at 17.06.44.png){width="562"}

![](images/Screenshot 2025-03-05 at 17.07.06.png){width="592"}

##### Analyze the sample 

we skipped testing on simulated data this time because we're lazy.

```{r}
data(Howell1)
d <- Howell1
d <-d[d$age>=18,]
dat <- list(
  W = d$weight,
  H = d$height,
  Hbar = mean(d$height),
  S=d$male + 1 #S = 1 female, S = 2 male
) 

m_SHW <- quap(
  alist(
    W ~dnorm(mu,sigma),
    mu <- a[S] +b[S]*(H-Hbar),
    a[S] ~ dnorm(60,10),
    b[S] ~dunif(0,1),
    sigma ~ dunif(0,10)
  ), data = dat
)
#getting a weird error on this saying I have bad priors?
```

Contrast now at each height -

1.  compute posterior predictive for women

2.  compute posterior predictive for men

3.  subtract 2 from 1

4.  plot distribution at each height (on right)

```{r}
xseq <- seq(from=130, to=190, len=50)

muF <- 
  link(m_adults2, data = list(S=rep(1,50), H=xseq, Hbar=mean(d$height)))
lines(xseq, apply(muF, 2, mean), lwd=3, col =2)
  
muM <- 
  link(m_adults2, data = list(S=rep(2,50), H=xseq, Hbar = mean(d$height)))
lines(xseq, apply(muM, 2, mean), lwd=3, col=4)
  
mu_contrast <- muF - muM
plot(NULL, xlim=range(xseq), ylim=c(-6,8), xlab="height (cm)",
     ylab = "weight contrast(F-M)")
for(p in c(0.5,0.6,0.7,0.8,0.9,0.99))
  shade(apply(mu_contrast, 2, PI, prob=p), xseq)
abline(h=0, lty=2)

```

I couldn't get the above code chunk to run some issue with m_adults2 that I don't have time or energy to resolve right now. but it should look like this

![](images/Screenshot 2025-03-05 at 17.22.02.png){width="518"}

NEARLY ALL OF THE CAUSAL EFFECT OF sex on weight acts through HEIGHT differences. that's the outcome of this exercise. basically, yeah, men weigh more, because men are taller.

summary thoughts from this chunk:

-   categorical varaiables show up a lot

-   we want to use index coding when we do categorical variables. we care about them because we want to stratify our estimates by the relevant categories

-   parameters are rarely the estimate themselves, we use samples to compute relevant contrasts.

-   always summarize as the LAST step!

-   we want the MEAN DIFFERENCE and not the DIFFERENCE OF MEANS. the whole

# Week 2 Reading Notes - Chapter 4
