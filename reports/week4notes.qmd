---
title: "week4notes"
format: html
toc: true
---

# Week 4 lecture notes

## Lecture 7 - Overfitting - Fitting Over and Under

in life there are typically infinite causes for finite data. the estimator might exist but might not be useful - we need to balance complexity and simplicity

2 main struggles -

-   struggle against causation - how to use causal assumptions to design estimators, contrast alternative models

-   struggle against data - how to make the estimators work

problems of prediction:

-   what function describes a scatter of points well (fitting, compression)?

-   what function EXPLAINS the points? (this is causal influence, what we focused on last week)

-   what would happen if we changed a point? (this is intervention)

-   what is the next observation from the same process? (prediction)

### leave one-out cross-validation

(a process to get an idea of how good our model is at predicting things - the predictive accuracy of the statistical procedure)

1.  drop one point

2.  fit line to remaining

3.  predict dropped point

4.  repeat (1-3) with next point

5.  score is error on dropped (summed up for all the points)

Bayesian cross-validation - because we're doing bayesian stats, we use the entire posterior distribution, not just a point prediction.

for simple models, more parameters improves fit to sample

but may reduce accuracy of predictions out of sample

most accurate model trades off flexibility with overfitting.

### Regularization

cross-validation can compare between models, but it doesn't necessarily help you pick a good one? regularization, which means picking better priors, is how you want to pick a good model.

overfitting depends upon the priors

skeptical priors have tighter variance, reduce flexibility

regularization - function finds regular features of process. the procedure of designing a model that makes good predictions

good priors are often tighter than you think!

![](images/Screenshot%202025-04-04%20at%2016.50.36.png)

your goal is not to just compress or encode the sample using the model - it is to make predictions. tighter priors make models that are less good at describing the data in the sample (less flexible!) but better it is as predicting data outside the sample. choosing skeptical priors that let the model learn regular features but not irregular features is called REGULARIZATION.

you can still make priors TOO tight:

![](images/Screenshot%202025-04-04%20at%2016.53.59.png){width="517"}

this makes it worse for the sample size (if you fed it more data it would get better)

Regularizing priors:

how do you choose the width of the prior?

for causal inference, use science. for pure prediction, you can tune the prior using cross-validation. many tasks are a mix of inference and prediction. No need to be perfect, just better

So I think summarizing the above: the more polynomial terms (wiggles in your line) the better the line is at describing the data you have but the worse it is at predicting new datapoints. The more polynomial terms you have, also, the more the selection of your priors makes a difference - the tighter the priors, the worse it is at describing the data you have, but the better it is at predicting (compared to less tight priors)

### penalty prediction

leave one out cross validation is rad but it makes you fit the same number of models as data points which is a LOT of effort: BUT! you can actually do it just from fitting one model! 2 relatively easy ways. these tools try to estimate the prediction penalty from a single model. they are PSIS and WAIC.

![](images/Screenshot%202025-04-11%20at%2013.24.21.png)

with a single model, what the above shows is that PSIS and WAIC don't get exactly the same answer as leave-one-out cross-validation but they get the same idea and they're pretty good! for big samples they do the trick without being a nightmare to run for tons of data points

![](images/Screenshot%202025-04-11%20at%2013.26.09.png)

I think what he's saying in the above is that you need your DAG and your solid causal understanding of the system to justify your model and statistical inference, you can't just get weird with a bunch of models and then run WAIC or PSIS or CV and be like YEAH THIS MODEL IS GOOD! and then let that justify its selection - you're just trying to use those tools to measure and manage overfitting. they're looking at prediction in the absence of intervention, not prediction in the PRESENCE of intervention (which is what would make it causal). People do this and its bAD!

A lot of the time making decisions about what model to use based on how well it predicts data leads you crazy astray because predictive criteria actually perform better when you're doing wacky causal stuff with your model (like including post-treatment variables, handling your confounds wrong, etC)

### dealing with outliers

outliers are still data points! don't wanna just ignore them or drop them! it's the model that's wrong, not the data - dropping the outliers doesn't fix anything, it just ignores the problem - your model is wrong, not the data. quantify the influence of each point using cross-validation (how much does the posterior dist. shift when you drop each observation), then use a mixture model

to quantify their importance, he recommends the PSIS k statistic or the WAIC "penalty term" - these quantify the influence of each point

#### Importance sampling (PSIS)

#### Information Criteria (WAIC)

you should never use AIC because it's eclipsed by WAIC, which is better and more accurate

## Lecture 8 - MCMC

randomness is used by scientists to do calculations

back to the marriage/divorce rate example from the south:

we haven't actually measured any of the things in that DAG, only their descendents by sampling:

![](images/Screenshot%202025-04-16%20at%2016.22.34.png){width="509"}

there's other stuff going on in the background too, like the fact that the population in different states effects how good we are at sampling, etc.

Another problem example - suppose you have some students in a class, and you want to know if the class is working to improve their knowledge. we can't actually assess their knowledge, just assess it using things like test scores. the test itself also has an influence on the score, and there's differences between peple that will influence their score and the knowledge:

![](images/Screenshot%202025-04-16%20at%2016.25.13.png){width="449"}

TODAY! we're going to compute the posterior using MCMC. we've talked about several options for that so far:

1.  analytical approach (often impossible - only w simplest problems, still awkward)

2.  grid approximation (always possible but MASSIVELY intensive, rarely practical for computing time)

3.  quadratic approximation (this is what we've been using so far, similar to maximum liklhood in frequentist stats, but while it's useful it makes some strong assumptions so has limited applications)

4.  Markov chain Monte Carlo (intensive, but widely applicable)

### King Markov and his people

![](images/Screenshot%202025-04-16%20at%2016.39.43.png){width="331"}

![](images/Screenshot%202025-04-16%20at%2016.40.04.png){width="391"}

![](images/Screenshot%202025-04-16%20at%2016.40.27.png){width="440"}

the chain is a sewuence of draws from a distribution

markov chain - history doesn't matter, just where you are now

monte carlo - refers to the random aspect (from gambling)

![](images/Screenshot%202025-04-16%20at%2016.41.11.png){width="509"}

we don't use the metropolis algorithm much but its a simple MCMC algorithm

today, MCMC is diverse, the metropolis algorithm has yielded to newer, more efficient algorithms. many innovations, best methods are now gradients - we'll use one called the "hamilton monte carlo":

take a quadratic formula, take its logarithm, multiple it by -1, and you get a bucket:

![](images/Screenshot%202025-04-16%20at%2016.48.46.png){width="490"}

![](images/Screenshot%202025-04-16%20at%2016.49.58.png){width="436"}

basically to do hamiltonian monte carlo, we're going to release a marble into the bowl and let it roll according to the physics. we'll do that a bunch of times and record eaxh of the stopping points, then flick it again and record the next

the step size of the path of the marble matter - big steps are a bit inefficient bc for some reason they make a lot of uturns and return to where they start. smaller steps take a longer time to explore the space and get a good overall picture of the shape of the bowl

we're just gonna use the math library that does all the caculus for us - what we need is the gradients (what's uphill what's downhill). we can write them ourselves using calculus, OR - we can use auto-diff (automatic differentiation) which will find the symbolic derivative of our model code.

used in many machine learning approaches - backpropogation is a special case.

the library we're gonna use is called Stan - this is just named after Stanislaw Ulam

### New Jersey Wine example

![](images/Screenshot%202025-04-17%20at%2008.17.33.png)

he said we don't NEED to stratify by judge here (no backdoors no confounds)

![](images/Screenshot%202025-04-17%20at%2008.18.21.png)

```{r}
library(rethinking)
data(Wines2012)
d <- Wines2012

dat <- list(
  S= standardize(d$score),
  J=as.numeric(d$judge),
  W=as.numeric(d$wine),
  X=ifelse(d$wine.amer==1,1,2),
  Z= ifelse(d$judge.amer==1,1,2)
)

mQ <- ulam(
  alist(
    S ~ dnorm(mu,sigma),
    mu <- Q[W],
    Q[W] ~dnorm(0,1),
    sigma ~ dexp(1)
  ), data = dat, chains =4, cores = 4)

precis(mQ,2)
```

Drawing the Markov Owl

complex machinery but lots of diagnostics

1.  trace plots

    ![](images/Screenshot%202025-04-17%20at%2008.26.49.png){width="519"}

    these are just tracing what the chain is up to for each of the Q values we ran in the model just now

    everything in the grey chunk is it warming up, the white regions are the samples that we're actually drawing from the posterior distribution.

    we fit four chains - need more than one chain to check convergence, which is when each change explores the correct distribution and they all explore the SAME distribution. ![](images/Screenshot%202025-04-17%20at%2008.30.38.png){width="461"}

    \^that's all the four chains layered on top of each other, THIS below is a bad chain - they're wandering around, they're not layering on top of each other, blah :(

    ![](images/Screenshot%202025-04-17%20at%2008.31.05.png){width="445"}

2.  trace rank plots

    ![](images/Screenshot%202025-04-17%20at%2008.33.19.png){width="400"}

    the above looks good, this would be bad:

    ![](images/Screenshot%202025-04-17%20at%2008.33.58.png){width="323"}

3.  R-hat convergence measure

    chain convergence diagnostic - it's like a variance ratio -

    if chains do converge, the beginning and the end of the chain are in the same region, and independent chains should explore the same region. R-hat is a ratio of variances - as total variance shrinks to average variance within chains, R-hat approaches 1. this is NOT a test, no guarantees from a value of R that you are getting convergence, but it helps.

4.  n_eff (number of effective samples)

    "how long would the chain be if each sample was independent of the one before it?"

    when samples are autocorrelated, you have fewer EFFECTIVE samples

5.  divergent transitions![](images/Screenshot%202025-04-17%20at%2008.56.28.png)

    now goin back to make the model more sophisticated:

![](images/Screenshot%202025-04-17%20at%2008.44.44.png)

```{r}
mQO <- ulam(
  alist(
    S~dnorm(mu,sigma),
    mu <- Q[W] + O[X],
    Q[W] ~ dnorm(0,1),
    O[X] ~dnorm(0,1),
    sigma ~dexp(1)
  ), data = dat, chains =4, cores = 4
)
plot(precis(mQo,2))
```

this code not working, but should generate this:

![](images/Screenshot%202025-04-17%20at%2008.48.41.png){width="408"}

now adding in two more parameters:

![](images/Screenshot%202025-04-17%20at%2008.49.09.png)

![](images/Screenshot%202025-04-17%20at%2008.50.05.png)

![](images/Screenshot%202025-04-17%20at%2008.52.26.png)

# Week 4 reading notes

## Chapter 7

## Chapter 8

## Chapter 9

# Homework
