---
title: "week4notes"
format: html
toc: true
---

# Week 4 lecture notes

## Lecture 7 - Overfitting - Fitting Over and Under

in life there are typically infinite causes for finite data. the estimator might exist but might not be useful - we need to balance complexity and simplicity

2 main struggles -

-   struggle against causation - how to use causal assumptions to design estimators, contrast alternative models

-   struggle against data - how to make the estimators work

problems of prediction:

-   what function describes a scatter of points well (fitting, compression)?

-   what function EXPLAINS the points? (this is causal influence, what we focused on last week)

-   what would happen if we changed a point? (this is intervention)

-   what is the next observation from the same process? (prediction)

### leave one-out cross-validation

(a process to get an idea of how good our model is at predicting things - the predictive accuracy of the statistical procedure)

1.  drop one point

2.  fit line to remaining

3.  predict dropped point

4.  repeat (1-3) with next point

5.  score is error on dropped (summed up for all the points)

Bayesian cross-validation - because we're doing bayesian stats, we use the entire posterior distribution, not just a point prediction.

for simple models, more parameters improves fit to sample

but may reduce accuracy of predictions out of sample

most accurate model trades off flexibility with overfitting.

### Regularization

cross-validation can compare between models, but it doesn't necessarily help you pick a good one? regularization, which means picking better priors, is how you want to pick a good model.

overfitting depends upon the priors

skeptical priors have tighter variance, reduce flexibility

regularization - function finds regular features of process. the procedure of designing a model that makes good predictions

good priors are often tighter than you think!

![](images/Screenshot%202025-04-04%20at%2016.50.36.png)

your goal is not to just compress or encode the sample using the model - it is to make predictions. tighter priors make models that are less good at describing the data in the sample (less flexible!) but better it is as predicting data outside the sample. choosing skeptical priors that let the model learn regular features but not irregular features is called REGULARIZATION.

you can still make priors TOO tight:

![](images/Screenshot%202025-04-04%20at%2016.53.59.png){width="517"}

this makes it worse for the sample size (if you fed it more data it would get better)

Regularizing priors:

how do you choose the width of the prior?

for causal inference, use science. for pure prediction, you can tune the prior using cross-validation. many tasks are a mix of inference and prediction. No need to be perfect, just better

So I think summarizing the above: the more polynomial terms (wiggles in your line) the better the line is at describing the data you have but the worse it is at predicting new datapoints. The more polynomial terms you have, also, the more the selection of your priors makes a difference - the tighter the priors, the worse it is at describing the data you have, but the better it is at predicting (compared to less tight priors)

### penalty prediction

leave one out cross validation is rad but it makes you fit the same number of models as data points which is a LOT of effort: BUT! you can actually do it just from fitting one model! 2 relatively easy ways. these tools try to estimate the prediction penalty from a single model. they are PSIS and WAIC.

![](images/Screenshot%202025-04-11%20at%2013.24.21.png)

with a single model, what the above shows is that PSIS and WAIC don't get exactly the same answer as leave-one-out cross-validation but they get the same idea and they're pretty good! for big samples they do the trick without being a nightmare to run for tons of data points

![](images/Screenshot%202025-04-11%20at%2013.26.09.png)

I think what he's saying in the above is that you need your DAG and your solid causal understanding of the system to justify your model and statistical inference, you can't just get weird with a bunch of models and then run WAIC or PSIS or CV and be like YEAH THIS MODEL IS GOOD! and then let that justify its selection - you're just trying to use those tools to measure and manage overfitting. they're looking at prediction in the absence of intervention, not prediction in the PRESENCE of intervention (which is what would make it causal). People do this and its bAD!

A lot of the time making decisions about what model to use based on how well it predicts data leads you crazy astray because predictive criteria actually perform better when you're doing wacky causal stuff with your model (like including post-treatment variables, handling your confounds wrong, etC)

### dealing with outliers

outliers are still data points! don't wanna just ignore them or drop them! it's the model that's wrong, not the data - dropping the outliers doesn't fix anything, it just ignores the problem - your model is wrong, not the data. quantify the influence of each point using cross-validation (how much does the posterior dist. shift when you drop each observation), then use a mixture model

to quantify their importance, he recommends the PSIS k statistic or the WAIC "penalty term" - these quantify the influence of each point

#### Importance sampling (PSIS)

#### Information Criteria (WAIC)

you should never use AIC because it's eclipsed by WAIC, which is better and more accurate

## Lecture 8 - MCMC

# Week 4 reading notes

## Chapter 7

## Chapter 8

## Chapter 9

# Homework
